import React, { useEffect, useRef, useState } from 'react'
import * as faceapi from 'face-api.js'
import { loadAllModels } from '../lib/loadModels.js'

export default function EmotionDetector({ onModelsLoaded }) {
  const videoRef = useRef(null)
  const canvasRef = useRef(null)
  const [status, setStatus] = useState('Loading modelsâ€¦')
  const [topText, setTopText] = useState(null)
  const lastChangeRef = useRef(Date.now())

  // Funny text replacements for each emotion
  const funnyTexts = {
    happy: [
      'This face just won the lottery ðŸŽ‰',
      'Smiling like free WiFi was found ðŸ˜',
      'Powered by chocolate ðŸ«',
      'Grinning like a meme template ðŸ˜‚',
      'Happy as a cat with a laser pointer ðŸ±'
    ],
    sad: [
      'Thinking about Monday mornings ðŸ˜¢',
      'Lost WiFi connection ðŸ“¶ðŸš«',
      'Missing the last slice of pizza ðŸ•',
      'Remembering old Facebook posts ðŸ“¸',
      'Crying over chopped onions ðŸ§…'
    ],
    angry: [
      'Someone ate their snacks ðŸ˜¡',
      'Laptop crashed without saving ðŸ’»ðŸ”¥',
      'Standing in a long queue ðŸ•',
      'Auto-correct betrayed them ðŸ“±',
      'Traffic jam face ðŸš—ðŸš—ðŸš—'
    ],
    surprised: [
      'Saw their exam results ðŸ˜²',
      'Someone brought donuts ðŸ©',
      'The code worked first try ðŸ¤¯',
      'Unexpected plot twist ðŸŽ¬',
      'Dog just started talking ðŸ¶'
    ],
    fearful: [
      'Watching a horror movie alone ðŸ‘»',
      'Heard a noise at 3 AM ðŸŒ™',
      'Checking electricity bill âš¡',
      'Trying roller coaster for first time ðŸŽ¢',
      'Remembered embarrassing moment ðŸ˜³'
    ],
    disgusted: [
      'Tasted pineapple on pizza ðŸðŸ•',
      'Smelled mystery fridge food ðŸ¥´',
      'Stepped on something squishy ðŸ‘Ÿ',
      'Read internet comments ðŸ“',
      'Saw socks with sandals ðŸ§¦ðŸ‘¡'
    ],
    neutral: [
      'Loadingâ€¦ please wait â³',
      'Thinking about lunch ðŸ”',
      'Face of pure WiFi stability ðŸ“¡',
      'Lost in deep thoughts ðŸ§˜',
      'The ultimate poker face ðŸŽ­'
    ]
  }

  useEffect(() => {
    let stream
    let raf = null
    let isMounted = true

    async function start() {
      try {
        setStatus('Loading modelsâ€¦')
        await loadAllModels('/models')
        onModelsLoaded?.()
        setStatus('Requesting cameraâ€¦')
        stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user' }, audio: false })
        if (!isMounted) return
        if (videoRef.current) {
          videoRef.current.srcObject = stream
          await videoRef.current.play()
          setStatus('Detectingâ€¦')
          detectLoop()
        }
      } catch (err) {
        console.error(err)
        setStatus('Error: ' + err.message)
      }
    }

    async function detectLoop() {
      const video = videoRef.current
      const canvas = canvasRef.current
      if (!video || !canvas) return

      const { videoWidth, videoHeight } = video
      canvas.width = videoWidth
      canvas.height = videoHeight

      const displaySize = { width: videoWidth, height: videoHeight }
      const opts = new faceapi.TinyFaceDetectorOptions({ inputSize: 224, scoreThreshold: 0.5 })

      const detections = await faceapi
        .detectAllFaces(video, opts)
        .withFaceLandmarks()
        .withFaceExpressions()

      const resized = faceapi.resizeResults(detections, displaySize)
      const ctx = canvas.getContext('2d')
      ctx.clearRect(0, 0, canvas.width, canvas.height)

      faceapi.draw.drawDetections(canvas, resized)
      faceapi.draw.drawFaceLandmarks(canvas, resized)

      if (resized.length > 0) {
        const exps = resized[0].expressions
        const [label] = Object.entries(exps).reduce((a, b) => (a[1] > b[1] ? a : b))

        // Only update funny text if 5s passed
        if (Date.now() - lastChangeRef.current >= 5000) {
          const pool = funnyTexts[label] || funnyTexts['neutral']
          const randomText = pool[Math.floor(Math.random() * pool.length)]
          setTopText(randomText)
          lastChangeRef.current = Date.now()
        }
      }

      raf = requestAnimationFrame(detectLoop)
    }

    start()

    return () => {
      isMounted = false
      if (raf) cancelAnimationFrame(raf)
      if (videoRef.current) videoRef.current.pause()
      if (stream) stream.getTracks().forEach((t) => t.stop())
    }
  }, [onModelsLoaded])

  return (
    <div className="stage" style={{ position: 'relative' }}>
      <video ref={videoRef} playsInline muted style={{ width: '100%' }} />
      <canvas ref={canvasRef} style={{ position: 'absolute', top: 0, left: 0 }} />
      <div
        className="badge"
        style={{
          position: 'absolute',
          bottom: 20,
          left: '50%',
          transform: 'translateX(-50%)',
          background: 'rgba(0,0,0,0.6)',
          color: '#fff',
          padding: '8px 16px',
          borderRadius: '12px',
          fontSize: '18px'
        }}
      >
        {topText || status}
      </div>
    </div>
  )
}
